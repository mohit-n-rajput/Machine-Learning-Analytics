{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee709b50",
   "metadata": {},
   "source": [
    "# 4 - Cleaning Real Data\n",
    "\n",
    "Now it's time for the real stuff. Let's use a load in a real dataset and discover our next steps together.\n",
    "\n",
    "In *Appendix A - Scrape & Build NBA Salary Dataset*, we create a NBA Player salary dataset by web scraping  [hoopshype.com](hoopshype.com). We won't cover web scraping here but you can run that notebook if you want to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae03fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "# import local utils.py\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b445d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERFROM_SCRAPE = True\n",
    "BASE_DIR = pathlib.Path().resolve()\n",
    "DATASET_PATH = BASE_DIR / 'datasets'\n",
    "INPUT_PATH = DATASET_PATH / 'nba-historical-salaries.csv'\n",
    "print(f'Dataset *{INPUT_PATH.name}* exists:', INPUT_PATH.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb544e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d24f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb16ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b947270",
   "metadata": {},
   "source": [
    "The above commands tell us a lot about this data already:\n",
    "- Finanical data\n",
    "- Columns with dollar strings need to be cleaned (`$`)\n",
    "- Rename columns for consistency\n",
    "- There's 14,549 records each with 5 data points.\n",
    "- `adj_salary` is given data. Does this mean adjusted in today's dollars? Is this accurate?\n",
    "\n",
    "After this assessment, let's get to work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75685eef",
   "metadata": {},
   "source": [
    "### Column consistency\n",
    "\n",
    "_How you do anything, is how you do everything._\n",
    "\n",
    "Let's start with the mundane task of committing to a consistent naming convention for our columns across our entire project here. \n",
    "\n",
    "Before we do, let's see the columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efef30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1506e8b2",
   "metadata": {},
   "source": [
    "If you're a seasoned programmer, you will notice the issue. If you're new to programming you might miss it. If you look at each column name you will see a subtle shift in how each column casing is done.\n",
    "\n",
    "Casing types? Yes, seriously. Here are a few options:\n",
    "\n",
    "- `PascalCase` -> `ThisIsPascalCase`\n",
    "- `camelCase` -> `thisIsCamelCase`\n",
    "- `snake_case` -> `this_is_snake_case`\n",
    "- `kebab-case` -> `this-is-kebab-case` (aka `slugified-string`, `spinal-case`)\n",
    "\n",
    "\n",
    "Since I use Python and create a lot of web applications, I tend to use `snake_case` or `kebab-case`. If you're a SQL database person, you'd probably use `PascalCase`. If you're from JavaScript, you'd probably use a lot of `camelCase`.\n",
    "\n",
    "Whatever format you use, just be consistent. Let's rename our columns using `snake_case`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e10f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install python-slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e0ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slugify import slugify\n",
    "\n",
    "def to_snake_case(val):\n",
    "    # in the future, this will be stored in\n",
    "    # utils.py in the courses/ directory\n",
    "    kebab_case = slugify(val)\n",
    "    return kebab_case.replace('-', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913dfbae",
   "metadata": {},
   "source": [
    "I like using the `python-slugify` package to consistently and reliably convert any string into a url-ready slug (aka `kebab-casing`). Once we have a `slug`/`kebab-case` we can just switch out the dashes (`-`) for underscores (`_`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a5cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_columns = df.columns\n",
    "new_columns = [to_snake_case(x) for x in old_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1366b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_mapping = dict(zip(old_columns, new_columns))\n",
    "new_column_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f12d515",
   "metadata": {},
   "source": [
    "> `zip` is a cool built in python feature that combines two lists of the same length. Once you use `dict` around them, it will turn the left side list into keys and the right side list into values associated by their indices. I remember `zip` like  a zipper on your pants, backpacks, luggage, etc; each size has \"teeth\" that corresponds to the other side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns=new_column_mapping, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999c2d41",
   "metadata": {},
   "source": [
    "## Cleaning Rows\n",
    "\n",
    "Now that we've renamed our columns, let's clean up our rows. In `utils.py` we have the function `dollar_str_to_float` which converts dollar strings into floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_row(row_series):\n",
    "    row_series['salary'] = utils.dollar_str_to_float(row_series['salary'])\n",
    "    row_series['adj_salary'] = utils.dollar_str_to_float(row_series['adj_salary'])\n",
    "    return row_series\n",
    "\n",
    "df_cleaned = df.copy().apply(clean_row, axis=1)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936bf0ca",
   "metadata": {},
   "source": [
    "I hope that your alarm bells are going off. We never covered `df.apply` we only covered `df['my_col'].apply`. What gives?\n",
    "\n",
    "When you run `.apply` on an entire DataFrame, you can modify each row as you see fit instead of just an entire column. Another way to write this would be to write:\n",
    "\n",
    "```python\n",
    "df_cleaned = df.copy().apply(clean_row, axis=1)\n",
    "df_cleaned['salary'] = df_cleaned['salary'].apply(utils.dollar_str_to_float)\n",
    "df_cleaned['adj_salary'] = df_cleaned['adj_salary'].apply(utils.dollar_str_to_float)\n",
    "```\n",
    "\n",
    "And that would be perfectly acceptable. But there's a major difference. And it's this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_row_2(row_series):\n",
    "    dollar_cols = ['salary', 'adj_salary']\n",
    "    for col in dollar_cols:\n",
    "        row_series[col] = utils.dollar_str_to_float(row_series[col])\n",
    "    return row_series\n",
    "\n",
    "df_cleaned_2 = df.copy().apply(clean_row_2, axis=1)\n",
    "df_cleaned_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d6a87",
   "metadata": {},
   "source": [
    "`clean_row_2` gives us a way to reduce complexity by iterating over the columns we want to adjust. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d48802",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_2['adj_salary'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d326000",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_per_year = df_cleaned_2['year_start'].value_counts(sort=False)\n",
    "players_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeaa9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_per_year.plot(title='Number of Players Per Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_salary_df = df_cleaned_2.copy()[['year_start', 'adj_salary']]\n",
    "adj_salaries_cumlative = adj_salary_df.groupby(\"year_start\").sum()\n",
    "\n",
    "adj_salaries_cumlative.plot(title='Adjusted Cumaltive Salaries Over Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe57cb2",
   "metadata": {},
   "source": [
    "Look at this two charts! The second appears to be out-pacing the first.\n",
    "\n",
    "- upward trend of number of players and salaries\n",
    "- What happend in 2019?\n",
    "- 2020 seams to be trending towards a massive year for player payments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc0131",
   "metadata": {},
   "source": [
    "The above dataset leaves me with a lot of questions:\n",
    "\n",
    "- Are these adjust salary numbers correct (they are from hypehoops.com)\n",
    "- Are the per-player salaries going up or just the top 5% of players?\n",
    "- How does a players' salary correlate to wins / losses / other stats?\n",
    "- How does a team (full of players) and their salaries correlate to wins / losses / other stats?\n",
    "- Do the audience metrics support these numbers? (In person, online, etc) In other words, is there really this much economic value being generated?\n",
    "\n",
    "Answers to these questions will inevitably leads to more questions which hopefully means more and better data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f826a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to samples dir\n",
    "\n",
    "# df_cleaned_2.to_csv('samples/4-player-salaries-cleaned.csv', index=False)\n",
    "\n",
    "# players_per_year.rename(columns={\"year_start\": \"players\"}, inplace=True)\n",
    "# players_per_year.to_csv('samples/4-player-salaries-per-year.csv', index_label='year', index=True)\n",
    "\n",
    "# adj_salaries_cumlative['adj_salary_$'] = adj_salaries_cumlative['adj_salary'].apply(utils.float_to_dollars)\n",
    "# adj_salaries_cumlative.rename(columns={\"year_start\": \"year\"}, inplace=True)\n",
    "# adj_salaries_cumlative.to_csv(\"samples/4-adj-salaries-cumlative-per-year.csv\", index_label=\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819325b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb71da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
