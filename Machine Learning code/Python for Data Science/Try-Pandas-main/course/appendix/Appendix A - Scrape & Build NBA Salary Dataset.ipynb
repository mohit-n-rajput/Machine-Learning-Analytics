{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08553e5b",
   "metadata": {},
   "source": [
    "# Appendix A - Scrape & Build NBA Salary Dataset\n",
    "The goal of this notebook is to prepare our course with a pre-existing dataset. The data cleaning is done in the course itself; this is meant only to create the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d279ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install requests requests-html matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5624667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from decimal import Decimal\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from requests_html import HTML\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3e439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERFROM_SCRAPE = True\n",
    "BASE_DIR = pathlib.Path().resolve().parent.parent\n",
    "COURSES_DIR = BASE_DIR / 'course'\n",
    "DATASET_PATH = COURSES_DIR / 'datasets'\n",
    "OUTPUT_PATH = DATASET_PATH / 'nba-historical-salaries.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c973adc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COURSES_DIR.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1554825f",
   "metadata": {},
   "source": [
    "For this dataset, we use `hoopshype.com`'s record of player salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc7326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://hoopshype.com/salaries/players/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d701732",
   "metadata": {},
   "source": [
    "`hoopshype.com`'s salary data starts in the 1990-1991 season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be7e83d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_start = 1990"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48e19f3",
   "metadata": {},
   "source": [
    "End scraping at last year's season (this year might not be available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25aac766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_end = datetime.datetime.now().year - 1\n",
    "year_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64accc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990 https://hoopshype.com/salaries/players/1990-1991/\n",
      "1991 https://hoopshype.com/salaries/players/1991-1992/\n",
      "1992 https://hoopshype.com/salaries/players/1992-1993/\n",
      "1993 https://hoopshype.com/salaries/players/1993-1994/\n",
      "1994 https://hoopshype.com/salaries/players/1994-1995/\n",
      "1995 https://hoopshype.com/salaries/players/1995-1996/\n",
      "1996 https://hoopshype.com/salaries/players/1996-1997/\n",
      "1997 https://hoopshype.com/salaries/players/1997-1998/\n",
      "1998 https://hoopshype.com/salaries/players/1998-1999/\n",
      "1999 https://hoopshype.com/salaries/players/1999-2000/\n",
      "2000 https://hoopshype.com/salaries/players/2000-2001/\n",
      "2001 https://hoopshype.com/salaries/players/2001-2002/\n",
      "2002 https://hoopshype.com/salaries/players/2002-2003/\n",
      "2003 https://hoopshype.com/salaries/players/2003-2004/\n",
      "2004 https://hoopshype.com/salaries/players/2004-2005/\n",
      "2005 https://hoopshype.com/salaries/players/2005-2006/\n",
      "2006 https://hoopshype.com/salaries/players/2006-2007/\n",
      "2007 https://hoopshype.com/salaries/players/2007-2008/\n",
      "2008 https://hoopshype.com/salaries/players/2008-2009/\n",
      "2009 https://hoopshype.com/salaries/players/2009-2010/\n",
      "2010 https://hoopshype.com/salaries/players/2010-2011/\n",
      "2011 https://hoopshype.com/salaries/players/2011-2012/\n",
      "2012 https://hoopshype.com/salaries/players/2012-2013/\n",
      "2013 https://hoopshype.com/salaries/players/2013-2014/\n",
      "2014 https://hoopshype.com/salaries/players/2014-2015/\n",
      "2015 https://hoopshype.com/salaries/players/2015-2016/\n",
      "2016 https://hoopshype.com/salaries/players/2016-2017/\n",
      "2017 https://hoopshype.com/salaries/players/2017-2018/\n",
      "2018 https://hoopshype.com/salaries/players/2018-2019/\n",
      "2019 https://hoopshype.com/salaries/players/2019-2020/\n",
      "2020 https://hoopshype.com/salaries/players/2020-2021/\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "if PERFROM_SCRAPE:\n",
    "    for year in range(year_start, year_end+1):\n",
    "        # NBA season spans 2 different calendar years\n",
    "        year_range = f\"{year}-{year+1}\"\n",
    "        # the lookup salary url is based on the above range\n",
    "        url = f\"{base_url}{year_range}/\"\n",
    "        # print year and url for manual review\n",
    "        print(year, url)\n",
    "        # perform lookup\n",
    "        r = requests.get(url)\n",
    "        # Convert response html text as a parsable object\n",
    "        html = HTML(html=r.text)\n",
    "        # Find the data table containing\n",
    "        table = html.find('table', first=True)\n",
    "        # table_data list holder\n",
    "        table_data = []\n",
    "        # iterate the table element and append all column values in each row\n",
    "        for el in table.element.getchildren():\n",
    "            for tr in el.getchildren():\n",
    "                row_data = []\n",
    "                for col in tr.getchildren():\n",
    "                    row_data.append(col.text_content().strip())\n",
    "                table_data.append(row_data)\n",
    "        # create the initial dataframe\n",
    "        init_df = pd.DataFrame(table_data)\n",
    "        # use the first row as the header\n",
    "        new_header = init_df.iloc[0]\n",
    "        # use everything after the first row as our dataset\n",
    "        init_df = init_df[1:]\n",
    "        # update header\n",
    "        init_df.columns = new_header\n",
    "\n",
    "        # attempt to rename columns, if it's avaiable\n",
    "        # otherwise, move to the next year lookup\n",
    "        try:\n",
    "            renamed_cols = {\n",
    "                \"Player\": 'player',\n",
    "                f\"{new_header[2]}\": \"salary\",\n",
    "                f\"{new_header[3]}\": \"adj_salary\"\n",
    "            }\n",
    "            init_df = init_df.rename(columns=renamed_cols)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # create \n",
    "        try:\n",
    "            df = init_df.copy()[['player', 'salary', 'adj_salary']]\n",
    "        except:\n",
    "            continue\n",
    "        # update dataset with year values \n",
    "        df['year-start'] = year\n",
    "        df['year-end'] = year + 1\n",
    "        # append this dataset to our group of datasets\n",
    "        dfs.append(df)\n",
    "        # slow down lookups to ensure our scraping doesn't overload\n",
    "        # hoopshype.com\n",
    "        time.sleep(1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86901e5",
   "metadata": {},
   "source": [
    "Convert our list of dataframes (ie season salaries) into our entire dataset via pandas concat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5417732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14549, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.concat(dfs) #[['player', 'year-start', 'year-end', 'salary', 'adj_salary']]\n",
    "dataset_df.reset_index(drop=True, inplace=True)\n",
    "dataset_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f44e8",
   "metadata": {},
   "source": [
    "Store file to our course data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "262b15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.to_csv(OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5697c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
